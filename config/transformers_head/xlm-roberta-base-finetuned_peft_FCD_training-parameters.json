{
"eval_steps": 1,
"evaluation_strategy": "epoch",
"logging_steps": 10,
"logging_strategy": "steps",
"num_train_epochs": 50,
"output_dir": "/nlp/models/transformers_head/xlm-roberta-base-finetuned_peft_FCD",
"per_device_train_batch_size": 16,
"per_device_eval_batch_size": 16,
"push_to_hub": false,
"report_to": "tensorboard",
"save_strategy": "no",
"warmup_ratio": 0.1
}